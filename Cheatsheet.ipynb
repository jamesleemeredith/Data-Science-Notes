{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 1\n",
    "## -Environment, Git and Bash Setup-\n",
    "\n",
    "### Setting up a Professional Data Science Environment - Configuring Git and Anaconda:\n",
    "### Connecting Your Terminal to GitHub:\n",
    "- git config --global user.name \"Your Name\"\n",
    "- git config --global user.email your@email.com\n",
    "\n",
    "### Cloning Repos:\n",
    "- git clone \n",
    "\n",
    "### Creating a New Repo Locally:\n",
    "- Create directory locally\n",
    "- git init\n",
    "- git add README.md\n",
    "- git commit -m \"first commit\"\n",
    "- git remote add origin url_goes_here\n",
    "\n",
    "OR \n",
    "- Create Repo on GitHub first (via forking or brand new on the site)\n",
    "- git clone inside desired directory\n",
    "\n",
    "### Other Useful Commands:\n",
    "- pwd\n",
    "- ls\n",
    "- cd <Location>\n",
    "- mkdir \n",
    "- <software> --version\n",
    "- git add, git commit, git push, git merge, git branch, git checkout, git status, git stash, and git log\n",
    "(Refer to Git Cheat sheet for more)\n",
    "- .gitignore (file type)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -PYTHON-"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis in Base Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Data From Files:\n",
    "!ls (in jupyter notebook)\n",
    "\n",
    "# Base Python:\n",
    "file_obj = open(\"file_path\")\n",
    "file_contents = file_obj.readlines()\n",
    "file_obj.close()\n",
    "# OR\n",
    "with open(\"file_path\") as file_obj:\n",
    "    file_contents = file_obj.readlines()\n",
    "# Writing Files:\n",
    "file_obj = open(\"file_path\", mode='w')\n",
    "file_obj.close()\n",
    "\n",
    "# CSVs:\n",
    "import csv\n",
    "with open(\"file_path\") as f:\n",
    "    data = list(csv.DictReader(f))\n",
    "\n",
    "#JSON:\n",
    "import json\n",
    "with open(\"file_path\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Parsing a JSON File:\n",
    "data.keys()\n",
    "# Drill down into the various keys (ex. data['data'][0])\n",
    "\n",
    "# Exploring JSON Schemas:\n",
    "data.keys()\n",
    "for key in data['albums'].keys():\n",
    "    print(key, type(data['albums'][key]))\n",
    "\n",
    "# Writing JSON To Memory\n",
    "json.dump"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis in Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv.DictReader to Pandas\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"file_path\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    data = list(reader)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Importing Data Directly into Pandas:\n",
    "df = pd.read_csv(\"data\", delimiter, skiprows, header)\n",
    "df = pd.read_excel()\n",
    "df = pd.read_json()\n",
    "df = pd.DataFrame.from_dict()\n",
    "\n",
    "# Exporting Data from Pandas:\n",
    "df.to_csv()\n",
    "df.to_excel()\n",
    "df.to_json()\n",
    "df.to_dict()\n",
    "\n",
    "# Assessing Data:\n",
    "df.head()\n",
    "df.tail()\n",
    "df.info()\n",
    "df.value_counts()\n",
    "df.shape\n",
    "df.columns\n",
    "df.index\n",
    "df.dtypes\n",
    "df.max()\n",
    "df.isna()# add .sum() after if you want the count\n",
    "df.sort_values()\n",
    "\n",
    "df.iloc[x,y]\n",
    "df.loc[0,'column']\n",
    "# OR Boolean indexing:\n",
    "df.loc[df['column'] < 12, ['column2']]\n",
    "# OR simply:\n",
    "df['column'] #columns\n",
    "df[2:4] # rows\n",
    "\n",
    "# Filtering Data:\n",
    "df[(df['column'] == \"value\") & (df['column2'] == 1950)]\n",
    "\n",
    "# Changing Values:\n",
    "df.loc[df['column'] > 10, 'column'] = 10\n",
    "\n",
    "#Creating new columns:\n",
    "df['new_column'] = 'value(s)'\n",
    "\n",
    "# Dropping Values\n",
    "df.drop(inplace=True)\n",
    "\n",
    "# Setting a New Index:\n",
    "df = df.set_index('column')\n",
    "df.reset_index()\n",
    "\n",
    "# Altering Data:\n",
    "# Using .map() to Transforms Values:\n",
    "# 1) Create a dictionary, 2):\n",
    "df.map('dictionary')\n",
    "# OR: Passing a function through .map():\n",
    "df.map(function) #OR df.apply(function)\n",
    "# Renaming Columns:\n",
    "df.rename(columns={'key':'value'},inplace=True)\n",
    "df.drop('DESC')\n",
    "# Changing Column Types:\n",
    "# Check types using df['column'].dtype\n",
    "df['column'] = df['column'].astype(int)\n",
    "# Converting Dates:\n",
    "pd.to_datetime(df['column'], format='%m/%d/%Y')\n",
    "# Applying Datetime Methods:\n",
    "df.dt.day_name()\n",
    "\n",
    "\n",
    "# Cleaning Data:\n",
    "df.isna()\n",
    "df.dropna()\n",
    "df.replace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis in Pandas\n",
    "\n",
    "# csv.DictReader to Pandas:\n",
    "import csv\n",
    "import pandas as pd\n",
    "with open(\"file_path\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    data = list(reader)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Importing Data Directly into Pandas:\n",
    "df = pd.read_csv(\"data\", delimiter, skiprows, header)\n",
    "df = pd.read_excel()\n",
    "df = pd.read_json()\n",
    "df = pd.DataFrame.from_dict()\n",
    "\n",
    "# Exporting Data from Pandas:\n",
    "df.to_csv()\n",
    "df.to_excel()\n",
    "df.to_json()\n",
    "df.to_dict()\n",
    "\n",
    "# Acessing Data about the Dataframe within Pandas:\n",
    "df.info()\n",
    "df.head()\n",
    "df.tail()\n",
    "df.value_counts()\n",
    "df.index\n",
    "df.columns\n",
    "df.dtypes\n",
    "df.shape\n",
    "df.iloc[2,3]\n",
    "df.loc[:,'column']\n",
    "# OR boolean indexing:\n",
    "df.loc[df['column'] < 12, ['column2']]\n",
    "# OR simply:\n",
    "df[0:4]\n",
    "df['column']\n",
    "\n",
    "# Filtering Data:\n",
    "df[(df['column'] == \"value\") & (df['column2'] == 1950)]\n",
    "\n",
    "# Changing Values:\n",
    "df.loc[df['column'] > 10, 'column'] = 10\n",
    "df['new_column'] = 'value'\n",
    "\n",
    "# Altering Rows and Columns:\n",
    "df.drop(inplace=True)\n",
    "\n",
    "# Setting a New Index:\n",
    "df = df.set_index('column')\n",
    "df.reset_index()\n",
    "\n",
    "# Altering Data:\n",
    "# Using .map() to Transforms Values:\n",
    "# 1) Create a dictionary, 2):\n",
    "df.map('dictionary')\n",
    "# OR: Passing a function through .map():\n",
    "df.map(function) #OR df.apply(function)\n",
    "# Renaming Columns:\n",
    "df.rename(columns={'key':'value'},inplace=True)\n",
    "df.drop('DESC')\n",
    "# Changing Column Types:\n",
    "# Check types using df['column'].dtype\n",
    "df['column'] = df['column'].astype(int)\n",
    "# Converting Dates:\n",
    "pd.to_datetime(df['column'], format='%m/%d/%Y')\n",
    "# Applying Datetime Methods:\n",
    "df.dt.day_name()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 2:\n",
    "\n",
    "## -SQL-\n",
    "\n",
    "### The Structure of SQL Databases:\n",
    "A SQL database is a high-level container containing one or more tables\n",
    "A diagram of the relationships between the tables in a database is none as an entity relationship diagram (ERD)\n",
    "Example of a table schema:\n",
    "\n",
    "~~~\n",
    "CREATE TABLE people (\n",
    "  id INTEGER PRIMARY KEY,\n",
    "  name TEXT,\n",
    "  age INTEGER,\n",
    "  email TEXT\n",
    ");\n",
    "~~~~\n",
    "\n",
    "Once you're connected to the database, you can then read, write, update, and delete data from its tables.\n",
    "These commands are called queries and are written using the SQL language.\n",
    "\n",
    "Primary Keys - a unique identifier for a table (example \"Student_Id\")\n",
    "Foreign keys - the primary key from a different (\"foreign\") table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to a SQLite database using Python:\n",
    "import sqlite3 \n",
    "conn = sqlite3.connect('data.sqlite')\n",
    "\n",
    "# Selecting Data with SQL:\n",
    "pd.read_sql(\"\"\"SELECT * FROM table;\"\"\", conn)\n",
    "# Without Using Pandas:\n",
    "cur.execute(\"\"\"SELECT name FROM sqlite_master WHERE type = 'table';\"\"\")\n",
    "\n",
    "# Retrieving Columns\n",
    "pd.read_sql(\"\"\"\n",
    "SELECT Col1\n",
    "  FROM table;\n",
    "  \"\"\", conn)\n",
    "# NOTE: can also use Col1.table instead of just Col1\n",
    "\n",
    "# Renaming Columns in Query Results using Aliases:\n",
    "pd.read_sql(\"\"\"\n",
    "SELECT firstName AS name\n",
    "  FROM employees;\n",
    "\"\"\", conn)\n",
    "\n",
    "# Binning Column Values Using CASE\n",
    "pd.read_sql(\"\"\"\n",
    "SELECT Col1\n",
    "       CASE\n",
    "       WHEN Col1 = \"Condition\" THEN \"Output1\"\n",
    "       ELSE \"Output2\"\n",
    "       END AS New_Column\n",
    "  FROM table;\n",
    "\"\"\", conn)\n",
    "\n",
    "# Using Built-In SQL Functions:\n",
    "pd.read_sql(\"\"\"\n",
    "SELECT upper(firstName) AS name_in_all_caps\n",
    "  FROM employees;\n",
    "\"\"\", conn)\n",
    "# Other Built-In SQL Functions:\n",
    "count()\n",
    "length()\n",
    "upper()\n",
    "substr()\n",
    "round()\n",
    "CAST()\n",
    "julianday()\n",
    "strftime()\n",
    "COUNT(DISTINCT)\n",
    "\n",
    "# Filtering Data Using WHERE:\n",
    "pd.read_sql(\"\"\"\n",
    "SELECT *\n",
    "  FROM table\n",
    " WHERE column = \"Condition\";\n",
    "\"\"\", conn)\n",
    "\n",
    "# Conditional Operators TO USE WITH WHERE:\n",
    "'''\n",
    "    != (\"not equal to\")\n",
    "    > (\"greater than\")\n",
    "    >= (\"greater than or equal to\")\n",
    "    < (\"less than\")\n",
    "    <= (\"less than or equal to\")\n",
    "    AND\n",
    "    OR\n",
    "    BETWEEN\n",
    "    IN\n",
    "    LIKE\n",
    "    IS/NOT NULL\n",
    "'''\n",
    "# Wildcards:\n",
    "'''\n",
    "% - Any number of characters\n",
    "_ - Exactly that many characters\n",
    "'''\n",
    "\n",
    "# Ordering Data:\n",
    "'''\n",
    "SELECT column(s)\n",
    "  FROM table_name\n",
    " ORDER BY column_name1,column_name2 (sort_order);''' # Where sort_order is ASC or DESC\n",
    "\n",
    "\n",
    "# Limiting Data:\n",
    "'''SELECT column(s)\n",
    "  FROM table_name\n",
    " LIMIT number;\n",
    "'''\n",
    "\n",
    "# Grouping Data:\n",
    "'''\n",
    "SELECT column, COUNT(*)\n",
    "FROM table\n",
    "GROUP BY column;\n",
    "'''\n",
    "# Grouping is commonly performed in combination with aggregations using the following functions:\n",
    "COUNT()\n",
    "MIN()\n",
    "MAX()\n",
    "SUM()\n",
    "AVG()\n",
    "# GROUP BY and The HAVING Clause:\n",
    "# The HAVING clause works similarly to the WHERE clause, except it is used to filter data selections on conditions after the GROUP BY clause.\n",
    "'''\n",
    "SELECT column, COUNT(*)\n",
    "  FROM table\n",
    "  GROUP BY column\n",
    "  HAVING condition\n",
    "  '''\n",
    "# Note that we can also use the WHERE and HAVING Clauses together for more complex filtering:\n",
    "'''\n",
    "SELECT column, COUNT(*)\n",
    "  FROM table\n",
    "  WHERE condition1\n",
    "  GROUP BY column\n",
    "  HAVING condition2;\n",
    "  '''\n",
    "\n",
    "# Database Admin 101\n",
    "# Creating a new database file\n",
    "import sqlite3 \n",
    "conn = sqlite3.connect('database.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Creating Tables\n",
    "cur.execute(\"\"\"CREATE TABLE cats (\n",
    "  id INTEGER PRIMARY KEY,\n",
    "  name TEXT,\n",
    "  age INTEGER,\n",
    "  breed TEXT )\n",
    "\"\"\")\n",
    "\n",
    "# Altering a Table:\n",
    "cur.execute('''ALTER TABLE table_name\n",
    "  ADD COLUMN column_name column_type;\n",
    "''')\n",
    "\n",
    "# INSERTING VALUES:\n",
    "cur.execute('''INSERT INTO cats (name, age, breed) \n",
    "  VALUES ('Maru', 3, 'Scottish Fold');\n",
    "''')\n",
    "\n",
    "# Updating Data:\n",
    "'''UPDATE [table name]\n",
    "    SET [column name] = [new value]\n",
    "    WHERE [column name] = [old value];\n",
    "'''\n",
    "# Deleting Data:\n",
    "'''\n",
    "DELETE FROM [table] WHERE [column] = [value];\n",
    "'''\n",
    "# Committing Changes to a Database:\n",
    "conn.commit()\n",
    "\n",
    "# --JOINS--:\n",
    "'''SELECT *\n",
    "FROM table1\n",
    "JOIN table2\n",
    "    ON table1.shared_column = table2.shared_column;'''\n",
    "# NOTE: We can use table aliases on the above to shorten the ON clause\n",
    "# Also Note: If shared_column is identically named in both tables, we can use the USING Clause instead:\n",
    "q = \"\"\"\n",
    "SELECT *\n",
    "FROM table1\n",
    "JOIN table2\n",
    "    USING(shared_column)\n",
    "LIMIT 10\n",
    ";\n",
    "\"\"\"\n",
    "# Types of JOINS: INNER JOIN (Selected by Default), LEFT JOIN, RIGHT JOIN, OUTER JOIN\n",
    "# NOTE: Beware of One-to-Many and Many-to-Many Joins ballooning your query results!\n",
    "\n",
    "# --SUBQUERIES--:\n",
    "# Subqueries can be used for the same tasks as JOINs, but also if you want to filter or order by an aggregate, but not select an aggregate.\n",
    "q = '''\n",
    "SELECT\n",
    "    shared_column\n",
    "    (\n",
    "        SELECT shared_column\n",
    "        FROM table 2\n",
    "        WHERE table2.shared_column = table2.shared_column\n",
    "    )\n",
    "FROM table 1;'''\n",
    "pd.read_sql(q, conn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SQL With Pandas:\n",
    "### Using .query()\n",
    ".query() is a built-in method in Pandas that allows querying of a dataframe.\n",
    "\n",
    "### Using Pandasql\n",
    "\n",
    "The pandasql library allows querying of Pandas Dataframes using SQL-stype syntax.\n",
    "In order to use pandasql, we need to start by importing a sqldf object from pandasql:\n",
    "<br>\n",
    "```from pandasql import sqldf```\n",
    "<br>\n",
    "Next, it's helpful to write a lambda function that will make it quicker and easier to write queries. Normally, you would have to pass in the global variables every time we use an object. In order to avoid doing this every time, here's how to write a lambda that does this for you:\n",
    "<br>\n",
    "```pysqldf = lambda q: sqldf(q, globals())```\n",
    "<br>\n",
    "To write a query, you just format it as a multi-line string!\n",
    "\n",
    "q = \"\"\"SELECT\n",
    "        m.date, m.beef, b.births\n",
    "     FROM\n",
    "        meats m\n",
    "     INNER JOIN\n",
    "        births b\n",
    "           ON m.date = b.date;\"\"\"\n",
    "\n",
    "In order to query DataFrames, you can just pass in the query string you've created to our sqldf object that you stored in pysqldf. This will return a DataFrame.\n",
    "<br>\n",
    "```results = pysqldf(q)```\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Statistical Distributions -"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Census vs Sample:\n",
    "Rarely, if ever, are we able to completely survey a population of interest. If we performed a complete survey of all data points, that would be a census. More often, data scientists work with a sample that represents a smaller subset of the population.\n",
    "### Descriptive statistics\n",
    "Statistical measures such as measures of central tendency (e.g. mean, median) and measures of spread (e.g. absolute deviation, standard deviation) used to describe the distribution of a given collection of data points\n",
    "<br>\n",
    "In mathematical notation, we often use different symbols for a given statistic depending on whether it applies to the population or a sample. Ex:\n",
    "<br>\n",
    "<br>\n",
    "Number\n",
    "<br>\n",
    "𝑁 is the number of individuals/cases in a population\n",
    "<br>\n",
    "𝑛 is the number of individuals/cases in sample\n",
    "<br>\n",
    "\n",
    "Mean\n",
    "𝜇 is the population mean (pronounced \"mu\")\n",
    "<br>\n",
    "𝑥¯ is the sample mean (pronounced \"x bar\")\n",
    "<br>\n",
    "\n",
    "Standard Deviation\n",
    "<br>\n",
    "𝜎 is the population standard deviation (pronounced \"sigma\")\n",
    "<br>\n",
    "𝑠 is the sample standard deviation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a sample of a dataframe:\n",
    "sample = df.sample(n=50, random_state=22)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "As the number of samples increases, the Average $\\bar{x}$ approaches $\\mu$:\n",
    "![Sample Distributions and X-bar](.\\images\\samplingdistributionsandxbar.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Distributions\n",
    "- Discrete distributions - distributions where the number of outcomes is finite and the outcome is a set of values.\n",
    "![A discrete distribution](.\\images\\discrete_distribution.png)\n",
    "    - When dealing with continuous data you use a Probability Mass Function (PMF)\n",
    "- Continuous distributions - a distribution of continuous values. \n",
    "![A continuous distribution](.\\images\\continuous_distribution.png)\n",
    "    - When dealing with continuous data, you use a Probability Density Function (PDF)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Discrete Distributions\n",
    "- The Bernoulli Distribution\n",
    "\n",
    "The Bernoulli distribution represents the probability of success for a certain experiment (the outcome being \"success or not\", so there are two possible outcomes). A coin toss is a classic example of a Bernoulli experiment with a probability of success 0.5 or 50%, but a Bernoulli experiment can have any probability of success between 0 and 1.\n",
    "- The Poisson Distribution\n",
    "\n",
    "The Poisson distribution represents the probability of events in a given time period when the overall rate of occurrence is constant. A typical example is pieces of mail. If your overall mail received is constant, the number of items received on a single day (or month) follows a Poisson distribution. Other examples might include visitors to a website, or customers arriving at a store, or clients waiting to be served in a queue.\n",
    "- The Uniform Distribution\n",
    "\n",
    "The uniform distribution occurs when all possible outcomes are equally likely. The dice example shown before follows a uniform distribution with equal probabilities for throwing values from 1 to 6. The dice example follows a discrete uniform distribution, but continuous uniform distributions exist as well.\n",
    "### Examples of Continuous Distributions\n",
    "- The Normal or Gaussian Distribution\n",
    "\n",
    "A normal distribution is the single most important distribution, you'll basically come across it very often. The normal distribution follows a bell shape and is a foundational distribution for many models and theories in statistics and data science. A normal distribution turns up very often when dealing with real-world data including heights, weights of different people, errors in some measurement or grades on a test. Our temperature example above follows a normal distribution as well!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
